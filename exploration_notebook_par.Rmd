---
title: "R Notebook"
output: html_notebook
---
#Text Reuse, Clustering

This notebook will explore text reuse in Indain treaties at the paragraph level and clustering at the paragraph and document level.First, some general information regarding Indian treaties is explored. 
```{r}
library(rvest)
library(magrittr)
library(tidyverse)
library(stringr)
library(pbapply)
library(parallel)
library(textreuse)
library(dplyr)
library(igraph)
library(ggplot2)
library(stringr)
library(data.table)
library(Matrix)
library(tokenizers)
library(text2vec)
library(broom)
library(apcluster)
```
###General Analysis


```{r}
#dtm2 is generated below.

parsed_treaties <- readRDS("parsed_treaties.rds")
addyear <- basename(parsed_treaties$file) %>% str_replace("\\.htm", ".txt")


parsed_treaties <- parsed_treaties %>% 
  mutate(document_id = basename(parsed_treaties$file) %>% str_replace("\\.htm", ".txt"))

#dtm2 <- dtm2 %>%
#  mutate(document_id = rownames(dtm2))
         
  
dtm_to_df <- function(x, words) {
  stopifnot(is.character(words))
  out <- as_tibble(as.data.frame(as.matrix(x[, words])))
  colnames(out) <- words
  ids <- str_replace_all(rownames(x), "\\.txt", "")
  ids <- str_split_fixed(ids, "-", n = 2)
  out %>% 
    mutate(document_id = ids[ , 1, drop = TRUE])
          }
  
words_of_interest <- c("horse", "indian", "death", "buffalo")

counts <- dtm_to_df(dtm2, words_of_interest) %>% 
  gather(word, count, -document_id) %>% 
  filter(count > 0)

item_years <- parsed_treaties %>% 
  select(document_id, year)

counts %>% 
  group_by(document_id, word) %>% 
  summarize(count = sum(count)) %>% 
  left_join(parsed_treaties, by = "document_id") %>% 
  group_by(year, word) %>% 
  summarize(count = sum(count)) %>% 
  ggplot(aes(x = year, y = count, color = word)) +
  geom_point() +
  geom_smooth(span = 0.1, se = FALSE) +
  labs(title = "A Title") + 
  xlim(1760,1890)
```


###Creating the Corpus without Minhashes
```{r createcorpus, cache=TRUE}
corpus <- TextReuseCorpus(list.files("C:/Users/Joshua/Documents/rdata/indian_treaties/treaty-paragraphs", 
                   pattern = "*.txt",
                   full.names = TRUE), tokenizer = tokenize_ngrams, n = 5,
                          progress = FALSE)
#saveRDS()
```

###Creating the Corpus with Minhashes

```{r createcorpus2, cache=TRUE}
minhash <- minhash_generator(n = 40, seed = 3552)

corpus2 <- TextReuseCorpus(list.files("C:/Users/Joshua/Documents/rdata/indian_treaties/treaty-paragraphs", 
                   pattern = "*.txt",
                   full.names = TRUE), tokenizer = tokenize_ngrams, n = 5,
                          minhash_func = minhash, keep_tokens = TRUE,
                          progress = FALSE)

wc <- wordcount(corpus2)

corpus2 <- corpus2[wc >= 40]

#saveRDS(corpus2, "minhashed_corpus_par_level.rds")
#corpus2 <- readRDS("minhashed_corpus_par_level.rds")
```



```{r}

lsh_threshold(h = 40, b = 20)

buckets <- lsh(corpus2, bands = 20, progress = FALSE)
buckets
```

###Finding Candidates
```{r}
#matches <- lsh_query(buckets, "19000630400")
#matches

candidates <- lsh_candidates(buckets)

#saveRDS(candidates,"candidates_par_level.rds")
#candidates <- readRDS("candidates_par_level.rds")
 
```

### Comparing Candidates
```{r}

similarities <-lsh_compare(candidates, corpus2, jaccard_similarity)
```

Histogram of similarities
```{r} 
hist(similarities$score, breaks = 100) 
```
Filtering out the candidates that turned out not to be matches. 
```{r}
matches <- filter(similarities, score> .2)
hist(matches$score, breaks = 100)
``` 

```{r}

```


```{r}
get_treaty <- function(x) {
  str_sub(x, 1, 7)
}
similarities$a %>% sample(10) %>% get_treaty()


similarities %>% 
  mutate(treaty_a = get_treaty(a),
         treaty_b = get_treaty(b)) %>% 
  count(treaty_a, treaty_b) %>% 
  arrange(desc(n)) %>% 
  filter(treaty_a != treaty_b) %>%
  View

paragraph_network <- similarities %>% 
  mutate(treaty_a = get_treaty(a),
         treaty_b = get_treaty(b)) %>% 
      filter(treaty_a != treaty_b)
  

#matches = similarities > 0.2

matches %>% 
  mutate(treaty_a = get_treaty(a),
         treaty_b = get_treaty(b)) %>% 
  count(treaty_a, treaty_b) %>% 
  arrange(desc(n)) %>% 
  filter(treaty_a != treaty_b) %>%
   View


fd_matches =  (matches %>% mutate(treaty_a = get_treaty(a),
         treaty_b = get_treaty(b)) %>% 
  count(treaty_a, treaty_b) %>% 
  arrange(desc(n)) %>% 
  filter(treaty_a != treaty_b)) 
```

##Networks

###Paragraph Level Borrowing
The following graph displays treaty borrowing at the paragraph level.
```{r}

sample_graph2 <- graph.data.frame(paragraph_network,directed = FALSE)

plot(sample_graph2, 
     layout = layout.auto,
    #vertex.label.cex = 0.7,
    vertex.label = NA,
    margin = -0.4,
    vertex.size = 1,
      main = "Network Graph of Paragraph Borrowing",
    edge.width = similarities$score)

```
###Decomposing the Paragraph Network

The following graph represents the largest cluster from above.
```{r}
components <-decompose(sample_graph2, mode = c("weak", "strong"), max.comps = NA,
  min.vertices = 3)

components_1_graph <- (components[[1]])

plot(components_1_graph,
     layout = layout.auto,
    #vertex.label.cex = 0.7,
    vertex.label = NA,
    margin = -0,
    vertex.size = 1,
    main = "Network Graph of Largest Paragraph Cluster",
    edge.width = similarities$score)
```

###Treaty Borrowing at the Document Level

The following are network graphs of the treaties with the edges representing the number of shared paragraphs. 
```{r}

sample_graph <- graph.data.frame(fd_matches,directed = FALSE)

plot(sample_graph, 
     layout = layout.auto,
    #vertex.label.cex = 0.7,
    vertex.label = NA,
    margin = -0.1,
     vertex.size = 1,
    main = "Network Graph of Treaty Borrowing",
    edge.width = fd_matches$n >10)
    
```
```{r}
plot(sample_graph, 
     layout = layout.auto,
    #vertex.label.cex = 0.7,
    vertex.label = NA,
    margin = -0.1,
     vertex.size = 1,
    main = "Network Graph of Treaty Borrowing",
    edge.width = fd_matches$n >5)
```
###Decomposing the Document Network
```{r}
components_whole <-decompose(sample_graph, mode = c("weak", "strong"), max.comps = NA,
  min.vertices = 3)

plot(components_whole[[7]],
     layout = layout.auto,
    vertex.label.cex = 0.6,
    #vertex.label = NA,
    margin = -0,
     vertex.size = 1,
    main = "Title",
    edge.width = fd_matches$n >3)

```
The largest cluster:
```{r}
components_whole_1 <- (components_whole[[1]])

plot(components_whole_1, 
     layout = layout.auto,
    vertex.label.cex = 0.6,
    #vertex.label = NA,
    margin = -0.2,
     vertex.size = 1,
    main = "Network Graph of Largest Treaty Cluster",
    edge.width = fd_matches$n >3)

```
Attempt to split the graph apart futher by removing vertices and the decomposing.

```{r}
broken_graph <- sample_graph

removed_vertices <- c("cre0155", "com0600", "cre0214", "wya0145", "iow0208", "pot0168", "mia0531", "sau0207",  "qua0160")
removed_edges <- c("osa0095|osa0167")

broken_graph <- delete_vertices(broken_graph, removed_vertices)                 #

#delete_edges(broken_graph, removed_edges) 
 
plot(broken_graph,
     layout = layout.auto,
    vertex.label.cex = 0.6,
    #vertex.label = NA,
    margin = -0.7,
    vertex.size = 1,
    main = "Network Graph of Largest Treaty Cluster",
    edge.width = fd_matches$n >3)

decomposed_broken_graph <-decompose(broken_graph, mode = c("weak", "strong"), max.comps = NA,
  min.vertices = 4)

plot(decomposed_broken_graph[[1]],
     layout = layout.auto,
    vertex.label.cex = 0.6,
    #vertex.label = NA,
    margin = -0.3,
     vertex.size = 1,
    main = "Title",
    edge.width = fd_matches$n >2)

```


Another decomposition with a higher number of minimum vertices.
```{r}
components_whole2 <-decompose(sample_graph, mode = c("weak", "strong"), max.comps = NA,
  min.vertices = 4)

plot(components_whole2[[8]],
     layout = layout.auto,
    vertex.label.cex = 0.6,
    #vertex.label = NA,
    margin = -0,
     vertex.size = 1,
    main = "Network Graph of Largest Treaty Cluster",
    edge.width = fd_matches$n >2)
```
The Sioux Cluster
```{r}
plot(components_whole2[[2]],
     layout = layout.auto,
    vertex.label.cex = 0.6,
    #vertex.label = NA,
    margin = -0,
     vertex.size = 1,
    main = "Sioux Bands Cluster, 1865",
    edge.width = fd_matches$n >3)
```

```{r}

#Attempt to bring in Years
document_id <- basename(parsed_treaties$file) %>% str_replace("\\.htm", "")

documents <- data_frame(document_id = fd_matches$treaty_a) %>% 
  left_join(parsed_treaties, by ="document_id")


documents <- data_frame(document_id = rownames(dtm)) %>% 
  left_join(us_subjects_moml, by = "document_id")  
  



filenames <- str_c("treaty-paragraphs/", doc_id, ".txt")

  
documents <- fd_matches(basename = fd_matches$treaty_a) %>% 
  leftjoin (parsed_treaties, by ="basename")
```

```{r}
documents <- data_frame(basename = fd_matches$treaty_a) %>% 
  left_join(parsed_treaties, by ="basename")
```

##PCA at the paragraph level. 
```{r}
#for paragraph level 
files <- list.files("C:/Users/Joshua/Documents/rdata/indian_treaties/treaty-paragraphs",                    pattern = "*.txt",
                  full.names = TRUE)


reader <- function(f) {
  require(stringr)
  n <- parsed_treaties$file %>% str_replace("\\.txt", "")
  doc <- readr::read_file(f)
  names(doc) <- n
  doc
}

it_files <- ifiles(files, reader = reader)
it_tokens <- itoken(it_files,
                   tokenizer = tokenizers::tokenize_words)

vocab <- create_vocabulary(it_tokens)

pruned_vocab <- prune_vocabulary(vocab, term_count_min = 10,
term_count_max = 50000)
vectorizer <- vocab_vectorizer(pruned_vocab)

dtm <- create_dtm(it_tokens, vectorizer)
rownames(dtm) <- basename(files) 

dtmsimilarities <- wordVectors::cosineSimilarity(dtm[1:1000, , drop = FALSE], 
                                              dtm[1:1000, , drop = FALSE])
dtmsimilarities %>% View

dtm2 <- as.matrix(dtm)

pca <- prcomp(dtm2, scale. = FALSE)
plot(pca)
augment(pca) %>% select(1:6) %>% as_tibble() %>% View

augment(pca) %>%
ggplot(aes(.fittedPC1, .fittedPC2)) + 
geom_point() 

#(for labels) geom_text_repel(aes(label = .rownames))
#saveRDS(pca,"pca_indian_treaties_paragraph_level.rds")
```


##K-Means at the Document Level
 
```{r}

treatycorpus <- list.files("C:/Users/Joshua/Documents/rdata/indian_treaties/treaty-complete",                    pattern = "*.txt",
                   full.names = TRUE)
 
reader <- function(f) {
  require(stringr)
  n <- basename(f) %>% str_replace("\\.txt", "")
  doc <- readr::read_file(f)
  names(doc) <- n
  doc
}

it_files2 <- ifiles(treatycorpus, reader = reader)
it_tokens2 <- itoken(it_files2,
                   tokenizer = tokenizers::tokenize_words)

vocab2 <- create_vocabulary(it_tokens2)

pruned_vocab2 <- prune_vocabulary(vocab2, term_count_min = 10,
term_count_max = 50000)
vectorizer2 <- vocab_vectorizer(pruned_vocab2)

dtm2 <- create_dtm(it_tokens2, vectorizer2)
rownames(dtm2) <- basename(treatycorpus)

parsed_treaties <- readRDS("parsed_treaties.rds")
addyear <- basename(parsed_treaties$file) %>% str_replace("\\.htm", ".txt")


parsed_treaties <- parsed_treaties %>% 
  mutate(document_id = basename(parsed_treaties$file) %>% str_replace("\\.htm", ".txt"))

km <- kmeans(dtm2, centers = 10)

k_clusters <- tibble(document_id = rownames(dtm2),
                     cluster = km$cluster) %>% 
  left_join(parsed_treaties, by = "document_id")

k_clusters %>% arrange(cluster) %>% View

plot(km$cluster)

ggplot(k_clusters, aes(388, 388)) + geom_point()
```


Affinity Propogation Clustering
```{r}

scores_clustering <- matches

section_names <- lsh_subset(scores_clustering)

lookup <- data_frame(section_names, index = 1:length(section_names))

lookup

scores_clustering <- scores_clustering %>% 
  left_join(lookup, by = c("a" = "section_names")) %>% 
  left_join(lookup, by = c("b" = "section_names")) 

scores_clustering

n <- length(section_names)
m <- sparseMatrix(i = scores_clustering$index.x,
                  j = scores_clustering$index.y,
                  x = scores_clustering$score,
                  dims = c(n, n), symmetric = TRUE)
colnames(m) <- section_names
rownames(m) <- section_names



cluster_cache <- "C:/Users/Joshua/Documents/rdata/indian_treaties/clusters.rds"
if (!file.exists(cluster_cache)) {
  timing <- system.time(
    clu <- apcluster(s = m,
                     maxits = 100e3, convits = 10e3,
                     q = 0,
                     lam = 0.975,
                     seed = 42325, 
                     includeSim = TRUE,
                     )
  )
  saveRDS(clu, cluster_cache)
 } else {
  clu <- readRDS(cluster_cache)
}

clusters <- clu@clusters 
names(clusters) <- names(clu@exemplars)
clusters <- lapply(clusters, names)


exemplars_corpus <- corpus2[names(clusters)]
exemplars_scores <- exemplars_corpus %>% 
  lsh(bands = 40) %>% 
  lsh_candidates() %>% 
  lsh_compare(exemplars_corpus, jaccard_similarity) %>% 
  arrange(desc(score))

boxplot(exemplars_scores$score)
hist(exemplars_scores$score)

```

Creating a Data Frame
```{r}
clusters_df <- clusters %>% 
  seq_along() %>% 
  lapply(function(i) {
    exemplar <- names(clusters)[i]
    doc <- clusters[[i]]
    data_frame(exemplar, doc, cluster_id = i)
  }) %>% 
  bind_rows() %>% 
  group_by(cluster_id) %>% 
  mutate(n = n()) %>% 
  ungroup() %>% 
  arrange(desc(n))
```




```{r}

#Check if Needed
join_threshold <- 0.15
exemplars_scores <- exemplars_scores %>% 
  filter(score >= 0.15)

join_clusters <- function(row) {
  exs <- c(row$a, row$b)
  minval <- which.min(extract_date(exs))
  exemplar <- exs[minval]
  duplicate <- exs[ifelse(minval == 1, 2, 1)]
  clusters[[exemplar]] <<- c(clusters[[exemplar]], clusters[[duplicate]])
  clusters[[duplicate]] <<- NULL
  dplyr::as_data_frame(row)
}

exemplars_scores %>% 
  rowwise() %>% 
  do(join_clusters(.))

hist(exemplars_scores$score)


```



```{r}
g <- make_ring(10) %>%
  delete_edges(seq(1, 9, by = 2))
g

plot(g)
g <- make_ring(10) %>%
  delete_edges("8|9")
plot(g)
```


