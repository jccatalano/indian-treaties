---
title: "R Notebook"
output: html_notebook
---

This notebook will explore text reuse in Indain treaties at the paragraph level. 
```{r}
library(rvest)
library(magrittr)
library(tidyverse)
library(stringr)
library(pbapply)
library(parallel)
library(textreuse)
library(dplyr)
library(igraph)
library(ggplot2)
library(stringr)
library(data.table)
library(Matrix)
library(tokenizers)
library(text2vec)
library(broom)
library(apcluster)
library(RPushbullet)
library(curl)
```

###Creating the Corpus without Minhashes
```{r createcorpus, cache=TRUE}
corpus <- TextReuseCorpus(list.files("C:/Users/Joshua/Documents/rdata/indian_treaties/treaty-paragraphs", 
                   pattern = "*.txt",
                   full.names = TRUE), tokenizer = tokenize_ngrams, n = 5,
                          progress = FALSE)
saveRDS()
```

###Creating the Corpus with Minhashes

```{r createcorpus2, cache=TRUE}
minhash <- minhash_generator(n = 40, seed = 3552)

corpus2 <- TextReuseCorpus(list.files("C:/Users/Joshua/Documents/rdata/indian_treaties/treaty-paragraphs", 
                   pattern = "*.txt",
                   full.names = TRUE), tokenizer = tokenize_ngrams, n = 5,
                          minhash_func = minhash, keep_tokens = TRUE,
                          progress = FALSE)

wc <- wordcount(corpus2)

corpus2 <- corpus2[wc >= 40]

#saveRDS(corpus2, "minhashed_corpus_par_level.rds")
#corpus2 <- readRDS("minhashed_corpus_par_level.rds")
```



```{r}

lsh_threshold(h = 40, b = 20)

buckets <- lsh(corpus2, bands = 20, progress = FALSE)
buckets
```

###Finding Candidates
```{r}
#matches <- lsh_query(buckets, "19000630400")
#matches

candidates <- lsh_candidates(buckets)

#saveRDS(candidates,"candidates_par_level.rds")
#candidates <- readRDS("candidates_par_level.rds")
 
```

### Comparing Candidates
```{r}

similarities <-lsh_compare(candidates, corpus2, jaccard_similarity)
```

Histogram of similarities
```{r} 
hist(similarities$score, breaks = 100) 
```
Filtering out the candidates that turned out not to be matches. 
```{r}
matches <- filter(similarities, score> .2)
hist(matches$score, breaks = 100)
``` 

```{r}

```


```{r}
get_treaty <- function(x) {
  str_sub(x, 1, 7)
}
similarities$a %>% sample(10) %>% get_treaty()


similarities %>% 
  mutate(treaty_a = get_treaty(a),
         treaty_b = get_treaty(b)) %>% 
  count(treaty_a, treaty_b) %>% 
  arrange(desc(n)) %>% 
  filter(treaty_a != treaty_b) %>%
  View


matches %>% 
  mutate(treaty_a = get_treaty(a),
         treaty_b = get_treaty(b)) %>% 
  count(treaty_a, treaty_b) %>% 
  arrange(desc(n)) %>% 
  filter(treaty_a != treaty_b) %>%
   View


fd_matches =  (matches %>% mutate(treaty_a = get_treaty(a),
         treaty_b = get_treaty(b)) %>% 
  count(treaty_a, treaty_b) %>% 
  arrange(desc(n)) %>% 
  filter(treaty_a != treaty_b)) 
```

Making a Network Graph
```{r}

sample_graph <- graph.data.frame(fd_matches,directed = FALSE)

plot(sample_graph, 
     layout = layout.auto,
    vertex.label.cex = 0.7,
    margin = -0.6,
     vertex.size = 1,
    edge.width = fd_matches$n >3)
    
  

```
```{r}
document_id <- basename(parsed_treaties$file) %>% str_replace("\\.htm", "")

documents <- data_frame(document_id = fd_matches$treaty_a) %>% 
  left_join(parsed_treaties, by ="document_id")


documents <- data_frame(document_id = rownames(dtm)) %>% 
  left_join(us_subjects_moml, by = "document_id")  
  



filenames <- str_c("treaty-paragraphs/", doc_id, ".txt")

  
documents <- fd_matches(basename = fd_matches$treaty_a) %>% 
  leftjoin (parsed_treaties, by ="basename")
```

```{r}
documents <- data_frame(basename = fd_matches$treaty_a) %>% 
  left_join(parsed_treaties, by ="basename")
```

PCA at the paragraph level. 
```{r}
#for paragraph level 
files <- list.files("C:/Users/Joshua/Documents/rdata/indian_treaties/treaty-paragraphs",                    pattern = "*.txt",
                  full.names = TRUE)

files <- parsed_treaties



get_treaty(files)


#Modified function
reader <- function(f) {
  require(stringr)
  n <- parsed_treaties$file %>% str_replace("\\.htm", "")
  doc <- readr::read_file(f)
  names(doc) <- n
  doc
}

it_files <- ifiles(files, reader = reader)
it_tokens <- itoken(it_files,
                   tokenizer = tokenizers::tokenize_words)

vocab <- create_vocabulary(it_tokens)

pruned_vocab <- prune_vocabulary(vocab, term_count_min = 10,
term_count_max = 50000)
vectorizer <- vocab_vectorizer(pruned_vocab)

dtm <- create_dtm(it_tokens, vectorizer)
rownames(dtm) <- basename(files) 

dtmsimilarities <- wordVectors::cosineSimilarity(dtm[1:1000, , drop = FALSE], 
                                              dtm[1:1000, , drop = FALSE])
dtmsimilarities %>% View

dtm2 <- as.matrix(dtm)

pca <- prcomp(dtm2, scale. = FALSE)
plot(pca)
augment(pca) %>% select(1:6) %>% as_tibble() %>% View

augment(pca) %>%
ggplot(aes(.fittedPC1, .fittedPC2)) + 
geom_point() 

#(for labels) geom_text_repel(aes(label = .rownames))
#saveRDS(pca,"pca_indian_treaties_paragraph_level.rds")
```


K-Means 
Cannot perform K-means because of memory constraints. 
```{r}
#km <- kmeans(dtm, centers = 20)

#k_clusters <- tibble(document_id = rownames(dtm),            cluster = km$cluster))
  

#k_clusters %>% arrange(cluster) %>% View
```


Affinity Propogation Clustering
```{r}

scores_clustering <- matches

section_names <- lsh_subset(scores_clustering)

lookup <- data_frame(section_names, index = 1:length(section_names))

lookup

scores_clustering <- scores_clustering %>% 
  left_join(lookup, by = c("a" = "section_names")) %>% 
  left_join(lookup, by = c("b" = "section_names")) 

scores_clustering

n <- length(section_names)
m <- sparseMatrix(i = scores_clustering$index.x,
                  j = scores_clustering$index.y,
                  x = scores_clustering$score,
                  dims = c(n, n), symmetric = TRUE)
colnames(m) <- section_names
rownames(m) <- section_names

cluster_cache <- "cache/clusters.rds"
if (!file.exists(cluster_cache)) {
  timing <- system.time(
    clu <- apcluster(s = m,
                     maxits = 100e3, convits = 10e3,
                     q = 0,
                     lam = 0.975,
                     seed = 42325, 
                     includeSim = TRUE,
                     )
  )
  saveRDS(clu, cluster_cache)
} else {
  clu <- readRDS(cluster_cache)
}

clusters <- clu@clusters 
names(clusters) <- names(clu@exemplars)
clusters <- lapply(clusters, names)

```





