---
title: "R Notebook"
output: html_notebook
---
#Indian Treaty Corpus Creation
This notebook details the process that I used to create my corpus of Indian treaties. Before I ran any of the code below, I used Wget to scrape the treaties from online (http://digital.library.okstate.edu/kappler/Vol2/Toc.htm). 

```{r}
library(rvest)
library(magrittr)
library(tidyverse)
library(stringr)
library(pbapply)
library(parallel)
library(textreuse)

```
I then loaded the package rvest in order to parse the htm documents created using Wget.
```{r}
test_treaty <- read_html("C:/Users/Joshua/Documents/wget_treaties/digital.library.okstate.edu/kappler/Vol2/treaties/apa0598.htm")

file <- "C:/Users/Joshua/Documents/wget_treaties/digital.library.okstate.edu/kappler/Vol2/treaties/apa0598.htm"
```
Here I tested a single treaty to ensure that I was able to extract the correct information from the CSS. Becasue there were two lines with the same CSS selector, I chose to keep only the first in order to create my "titles." The `str_c(collapse = " ")` combined the numerous body paragraphs into a single character. 
```{r}
title <- test_treaty %>%
  html_nodes("h4 center") %>%
  html_text()

title <- title[c(1)] 


treaty_descpription <- test_treaty %>%
  html_nodes("p i") %>%
  html_text() %>% 
  str_c(collapse = " ")

treaty_descpription

body <- test_treaty %>%
  html_nodes("body p") %>%
  html_text() #%>% 
    #str_c(collapse = " ")

body <- body[-1]

body <- str_c(body, collapse = " ")

year <- test_treaty %>%
   html_nodes("h4 center") %>%
   html_text () %>%
   str_extract("\\d{4}") %>%
   as.integer()

year <-year[c(1)]

```
Here I modified a parser function from Lincoln Mullen's Github repository, "gulag-names," to use on my list of files and turn them into a data frame.
```{r}
parser <- function(file) {
  message("Parsing ", file)
  trt <- read_html(file) 
   
  title <- trt %>%
  html_nodes("h4 center") %>%
  html_text()
  
  title <- title[1]

  treaty_descpription <- trt %>%
  html_nodes("p i") %>%
  html_text()%>%
  str_c(collapse= " ")


  body <- trt %>%
  html_nodes("body p") %>%
  html_text() 

body <- body[-1]

body <- str_c(body, collapse= "\n\n") 

  
  year <- trt %>%
   html_nodes("h4 center") %>%
   html_text () %>%
   str_extract("\\d{4}") %>%
   as.integer()

year <-year[1]
  
  data_frame(file = file, title = title, body = body,
             description = treaty_descpription, year = year)
  
}
```
Now, I finally loaded all of the treaties and ran them through the parser function shown above.
```{r}
# file_to_df <- failwith(NULL, parser)

treaty_filenames <- list.files("C:/Users/Joshua/Documents/wget_treaties/digital.library.okstate.edu/kappler/Vol2/treaties/",
                               pattern = "\\.htm",
                               full.names = TRUE)

parsed <- map_df(treaty_filenames, parser)


```
The end result is a data frame titled `parsed` that contains four columns corresponding to the elements that the parser function extracted (title, treaty_description, body) and the file. 


##Text Reuse Exploraiton
Now that the treaties have been scraped from the internet, the important information has been extracted, and the needed informaiton has been put into a data frame, I can create my corpus for my text reuse exploration. 

Below, I created the corpus out of the body columns of the `parsed` data frame. I chose to use the file names as the names of the documents because they were unique. For example, using the treaty names would result in several files containing the same name (this is because some nations signed multiple treaties with the United States in the same year). 
```{r}
# n <- basename(f) %>% stringr::str_replace("\\.txt", "")
# docs <- parsed$body
# names(doc) <-basename((docs)) 
                        
# (docs, "C:/Users/Joshua/Documents/wget_treaties/digital.library.okstate.edu/kappler/Vol2/treaties/", " ")


docs <- parsed$body
names(docs) <- basename(parsed$file)
corpus <- TextReuseCorpus(text = docs, meta = list(parsed$title),
                          tokenizer = tokenize_ngrams, n = 5,                                               progress = FALSE)
```
###Pairwise Comparison
Now, that I had a corpus, I started to run some comparisons starting with a pairwise comparision that would provide a jaccard similarity score. 
```{r}

comparisons <- pairwise_compare(corpus, jaccard_similarity, progress = FALSE)
comparisons[1:4, 1:4]


```
Here the matrix is converted to a data frame and only the results with a score greater than 0.1 are shown. 

```{r}
candidates <- pairwise_candidates(comparisons)

candidates[candidates$score > 0.2,]

hist(candidates$score,  ylim=c(0,80),breaks = 200)
```

##Minhash
I am going to use hashes to reduce down the number of comparisions computed by eliminating uneccessary comparisons (texts matched against themselves). This requires creating a new corpus.  
```{r}

minhash <- minhash_generator(n = 240, seed = 3552)

corpus2 <- TextReuseCorpus(text = docs, meta = list(parsed$title),
                          tokenizer = tokenize_ngrams, n = 5,
                          minhash_func = minhash, keep_tokens = TRUE,
                          progress = FALSE)

```
```{r}

head(minhashes(corpus2[[1]]))
length(minhashes(corpus2[[1]]))
```
```{r}
lsh_threshold(h = 100, b = 50)


buckets <- lsh(corpus2, bands = 80, progress = FALSE)
buckets
```
```{r}
baxter_matches <- lsh_query(buckets, "apa0598.htm")
baxter_matches

candidates <- lsh_candidates(buckets)
candidates
```
```{r}
lsh_compare(candidates, corpus2, jaccard_similarity, progress = FALSE)

similarities <-lsh_compare(candidates, corpus2, jaccard_similarity, progress = FALSE)
```

Histogram of similarities
```{r} 
hist(similarities$score, breaks = 10) 


```




