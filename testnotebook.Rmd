---
title: "R Notebook"
output: html_notebook
---
#Indian Treaty Corpus Creation
This notebook details the process that I used to create my corpus of Indian treaties. Before I ran any of the code below, I used Wget to scrape the treaties from online (http://digital.library.okstate.edu/kappler/Vol2/Toc.htm). 

```{r}
library(rvest)
library(magrittr)
library(tidyverse)
library(stringr)
library(pbapply)
library(parallel)
library(textreuse)

```
I then loaded the package rvest in order to parse the htm documents created using Wget.
```{r}
test_treaty <- read_html("C:/Users/Joshua/Documents/wget_treaties/digital.library.okstate.edu/kappler/Vol2/treaties/apa0598.htm")

file <- "C:/Users/Joshua/Documents/wget_treaties/digital.library.okstate.edu/kappler/Vol2/treaties/apa0598.htm"
```

```{r}
title <- test_treaty %>%
  html_nodes("h4 center") %>%
  html_text()

title <- title[c(1)] 


treaty_descpription <- test_treaty %>%
  html_nodes("p i") %>%
  html_text() %>% 
  str_c(collapse = " ")

treaty_descpription

body <- test_treaty %>%
  html_nodes("body p") %>%
  html_text() %>% 
  str_c(collapse = " ")

body


```

```{r}
parser <- function(file) {
  message("Parsing ", file)
  trt <- read_html(file) 
   
  title <- trt %>%
  html_nodes("h4 center") %>%
  html_text()
  
  title <- title[1]

  treaty_descpription <- trt %>%
  html_nodes("p i") %>%
  html_text()%>%
  str_c(collapse= " ")


  body <- trt %>%
  html_nodes("body p") %>%
  html_text() %>%
  str_c(collapse= "\n\n")
  
  data_frame(file = file, title = title, body = body,
             description = treaty_descpription)
  
}
```

```{r}
# file_to_df <- failwith(NULL, parser)

treaty_filenames <- list.files("C:/Users/Joshua/Documents/wget_treaties/digital.library.okstate.edu/kappler/Vol2/treaties/",
                               pattern = "\\.htm",
                               full.names = TRUE)

parsed <- map_df(treaty_filenames, parser)


```

##Text Reuse Exploraiton
```{r}
docs <- parsed$body
names(docs) <- basename(parsed$title)
corpus <- TextReuseCorpus(text = docs, meta = list(parsed$title),
                          tokenizer = tokenize_ngrams, n = 5,                                               progress = FALSE)
```
###Pairwise Comparison
```{r}

comparisons <- pairwise_compare(corpus, jaccard_similarity, progress = FALSE)
comparisons[1:4, 1:4]


```
Here the matrix is converted to a data frame and only the results with a score greater than 0.1 are kept. 

```{r}
candidates <- pairwise_candidates(comparisons)
candidates[candidates$score > 0.1, ]


```

##Minhash
I am going to use hashes to reduce down the number of comparisions computed by eliminating uneccessary comparisons (texts matched against themselves). This requires created a new corpus.  
```{r}

minhash <- minhash_generator(n = 240, seed = 3552)



docs <- parsed$body

names(docs) <- basename(parsed$title)

corpus2 <- TextReuseCorpus(text = docs, meta = list(parsed$title),
                          tokenizer = tokenize_ngrams, n = 5,
                          minhash_func = minhash, keep_tokens = TRUE,
                          progress = FALSE)

```
```{r}

head(minhashes(corpus2[[1]]))
length(minhashes(corpus2[[1]]))
```
```{r}
lsh_threshold(h = 100, b = 50)


buckets <- lsh(corpus2, bands = 80, progress = FALSE)
buckets
```
```{r}
baxter_matches <- lsh_query(buckets, "19000630400")
baxter_matches
candidates <- lsh_candidates(buckets)
candidates
```






